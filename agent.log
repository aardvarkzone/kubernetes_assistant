2024-10-31 14:22:51,989 INFO - Received query: Which namespace is the harbor service deployed to?
2024-10-31 14:22:51,996 DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are an AI assistant that interprets Kubernetes-related queries and returns structured JSON responses.\nYour goal is to generate accurate \'kubectl\' commands for Kubernetes clusters when possible. Each command should directly retrieve the requested data if available.\nIf a query does not require a Kubernetes command, provide a general response in JSON format.\n\nReturn responses in this JSON format:\n- \'kubectl_command\': The kubectl command to run to satisfy the query.\n\nIf no command is possible, respond instead with:\n- \'general_response\': A concise answer to the question or helpful guidance.\n\nGuidelines for generating commands:\n1. **Counting Resources**:\n   - Avoid using `wc -l` for counting, as it can be inaccurate. Instead, retrieve JSON output and count items programmatically.\n   - Example: For \'How many pods are there?\', respond with:\n     {"kubectl_command": "kubectl get pods --all-namespaces -o json --kubeconfig ~/.kube/config | jq \'.items | length\'"}\n\n2. **Querying Resource Status**:\n   - Use `-o=jsonpath` to query specific fields like `.status`, `.spec`, or `.metadata`. For instance, to get the status of a service or pod, target `.status.phase` or `.status.loadBalancer` as appropriate.\n   - Example: For \'What is the status of harbor registry?\', respond with:\n     {"kubectl_command": "kubectl get svc harbor-registry -o=jsonpath=\'{.status}\' --kubeconfig ~/.kube/config"}\n\n3. **Accessing IPs and Ports**:\n   - To get a pod or service\'s IP, use `.status.podIP` or `.status.loadBalancer.ingress[0].ip`.\n   - For container or service ports, access `.spec.ports[]` and look for `targetPort` or `containerPort`.\n   - Example: For \'What is the container port for harbor-core?\', respond with:\n     {"kubectl_command": "kubectl get pods -l app=harbor-core -o=jsonpath=\'{.items[0].spec.containers[0].ports[0].containerPort}\' --kubeconfig ~/.kube/config"}\n\n4. **Handling Probes (Readiness/Liveness)**:\n   - Use `.spec.containers[].readinessProbe` or `.livenessProbe` for probe paths, often located under `httpGet.path`.\n   - Example: For \'What is the readiness probe path for the harbor core?\', respond with:\n     {"kubectl_command": "kubectl get pods -l app=harbor-core -o=jsonpath=\'{.items[0].spec.containers[0].readinessProbe.httpGet.path}\' --kubeconfig ~/.kube/config"}\n\n5. **Accessing PostgreSQL Configuration Data**:\n   - For questions requiring data outside of kubectl commands (e.g., environment variables or database configuration), use `kubectl exec` to access pods directly.\n   - Example: For \'What is the name of the database in PostgreSQL used by harbor?\', respond with:\n     {"kubectl_command": "kubectl exec -n harbor $(kubectl get pod -n harbor -l app=harbor-database -o jsonpath=\'{.items[0].metadata.name}\' --kubeconfig ~/.kube/config) --kubeconfig ~/.kube/config -- printenv POSTGRES_DB"}\n\nRespond only with the JSON response without extra commentary.'}, {'role': 'user', 'content': 'Interpret this query: Which namespace is the harbor service deployed to?'}], 'model': 'gpt-4', 'max_tokens': 100, 'temperature': 0.3}}
2024-10-31 14:22:51,997 DEBUG - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2024-10-31 14:22:51,997 DEBUG - close.started
2024-10-31 14:22:51,998 DEBUG - close.complete
2024-10-31 14:22:51,998 DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-10-31 14:22:52,068 DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1080303a0>
2024-10-31 14:22:52,069 DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x106b76440> server_hostname='api.openai.com' timeout=5.0
2024-10-31 14:22:52,100 DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x108030370>
2024-10-31 14:22:52,100 DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-10-31 14:22:52,101 DEBUG - send_request_headers.complete
2024-10-31 14:22:52,101 DEBUG - send_request_body.started request=<Request [b'POST']>
2024-10-31 14:22:52,101 DEBUG - send_request_body.complete
2024-10-31 14:22:52,101 DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-10-31 14:22:54,660 DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 31 Oct 2024 21:22:54 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-wbjdefig2magqgot38fgvt8p'), (b'openai-processing-ms', b'2307'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'10000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'9159'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'5.046s'), (b'x-request-id', b'req_fb31a397afc415cf5b32fe6f074c6d1e'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8db6b3f3cb835220-LAX'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-10-31 14:22:54,661 INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-10-31 14:22:54,661 DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-10-31 14:22:54,661 DEBUG - receive_response_body.complete
2024-10-31 14:22:54,661 DEBUG - response_closed.started
2024-10-31 14:22:54,662 DEBUG - response_closed.complete
2024-10-31 14:22:54,662 DEBUG - HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Thu, 31 Oct 2024 21:22:54 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-wbjdefig2magqgot38fgvt8p', 'openai-processing-ms': '2307', 'openai-version': '2020-10-01', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '10000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '9159', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '5.046s', 'x-request-id': 'req_fb31a397afc415cf5b32fe6f074c6d1e', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '8db6b3f3cb835220-LAX', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2024-10-31 14:22:54,662 DEBUG - request_id: req_fb31a397afc415cf5b32fe6f074c6d1e
2024-10-31 14:22:54,662 DEBUG - GPT-4 response (variant 1): {"kubectl_command": "kubectl get svc -l app=harbor -o=jsonpath='{.items[0].metadata.namespace}' --kubeconfig ~/.kube/config"}
2024-10-31 14:22:54,663 ERROR - Error processing query: handle_k8s_query() missing 1 required positional argument: 'query'
Traceback (most recent call last):
  File "/Users/advaitarumugam/Cleric/main.py", line 48, in create_query
    answer = handle_k8s_query(structured_query)
TypeError: handle_k8s_query() missing 1 required positional argument: 'query'
2024-10-31 14:22:54,663 INFO - 127.0.0.1 - - [31/Oct/2024 14:22:54] "[35m[1mPOST /query HTTP/1.1[0m" 500 -
2024-10-31 14:23:51,052 INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:8000
 * Running on http://100.64.11.221:8000
2024-10-31 14:23:51,052 INFO - [33mPress CTRL+C to quit[0m
2024-10-31 14:23:53,539 INFO - Received query: Which namespace is the harbor service deployed to?
2024-10-31 14:23:53,549 DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are an AI assistant that interprets Kubernetes-related queries and returns structured JSON responses.\nYour goal is to generate accurate \'kubectl\' commands for Kubernetes clusters when possible. Each command should directly retrieve the requested data if available.\nIf a query does not require a Kubernetes command, provide a general response in JSON format.\n\nReturn responses in this JSON format:\n- \'kubectl_command\': The kubectl command to run to satisfy the query.\n\nIf no command is possible, respond instead with:\n- \'general_response\': A concise answer to the question or helpful guidance.\n\nGuidelines for generating commands:\n1. **Counting Resources**:\n   - Avoid using `wc -l` for counting, as it can be inaccurate. Instead, retrieve JSON output and count items programmatically.\n   - Example: For \'How many pods are there?\', respond with:\n     {"kubectl_command": "kubectl get pods --all-namespaces -o json --kubeconfig ~/.kube/config | jq \'.items | length\'"}\n\n2. **Querying Resource Status**:\n   - Use `-o=jsonpath` to query specific fields like `.status`, `.spec`, or `.metadata`. For instance, to get the status of a service or pod, target `.status.phase` or `.status.loadBalancer` as appropriate.\n   - Example: For \'What is the status of harbor registry?\', respond with:\n     {"kubectl_command": "kubectl get svc harbor-registry -o=jsonpath=\'{.status}\' --kubeconfig ~/.kube/config"}\n\n3. **Accessing IPs and Ports**:\n   - To get a pod or service\'s IP, use `.status.podIP` or `.status.loadBalancer.ingress[0].ip`.\n   - For container or service ports, access `.spec.ports[]` and look for `targetPort` or `containerPort`.\n   - Example: For \'What is the container port for harbor-core?\', respond with:\n     {"kubectl_command": "kubectl get pods -l app=harbor-core -o=jsonpath=\'{.items[0].spec.containers[0].ports[0].containerPort}\' --kubeconfig ~/.kube/config"}\n\n4. **Handling Probes (Readiness/Liveness)**:\n   - Use `.spec.containers[].readinessProbe` or `.livenessProbe` for probe paths, often located under `httpGet.path`.\n   - Example: For \'What is the readiness probe path for the harbor core?\', respond with:\n     {"kubectl_command": "kubectl get pods -l app=harbor-core -o=jsonpath=\'{.items[0].spec.containers[0].readinessProbe.httpGet.path}\' --kubeconfig ~/.kube/config"}\n\n5. **Accessing PostgreSQL Configuration Data**:\n   - For questions requiring data outside of kubectl commands (e.g., environment variables or database configuration), use `kubectl exec` to access pods directly.\n   - Example: For \'What is the name of the database in PostgreSQL used by harbor?\', respond with:\n     {"kubectl_command": "kubectl exec -n harbor $(kubectl get pod -n harbor -l app=harbor-database -o jsonpath=\'{.items[0].metadata.name}\' --kubeconfig ~/.kube/config) --kubeconfig ~/.kube/config -- printenv POSTGRES_DB"}\n\nRespond only with the JSON response without extra commentary.'}, {'role': 'user', 'content': 'Interpret this query: Which namespace is the harbor service deployed to?'}], 'model': 'gpt-4', 'max_tokens': 100, 'temperature': 0.3}}
2024-10-31 14:23:53,570 DEBUG - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2024-10-31 14:23:53,570 DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-10-31 14:23:53,608 DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x106ae9c00>
2024-10-31 14:23:53,608 DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1056aa440> server_hostname='api.openai.com' timeout=5.0
2024-10-31 14:23:53,629 DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x106ae9bd0>
2024-10-31 14:23:53,630 DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-10-31 14:23:53,630 DEBUG - send_request_headers.complete
2024-10-31 14:23:53,630 DEBUG - send_request_body.started request=<Request [b'POST']>
2024-10-31 14:23:53,631 DEBUG - send_request_body.complete
2024-10-31 14:23:53,631 DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-10-31 14:23:56,619 DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 31 Oct 2024 21:23:56 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-wbjdefig2magqgot38fgvt8p'), (b'openai-processing-ms', b'2698'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'10000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'9159'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'5.046s'), (b'x-request-id', b'req_0f219c9ba15b999ec562c928fcf0bd16'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=sozdZa5I8bmhvXm1yfV9y78dn462lgKeInfhtd6zEIQ-1730409836-1.0.1.1-uhB86RBRbc2JHec4XttFIKfilIhfSUe3iebh7ijw.giEMgc4HzrSfchCfZFUrG_uu7jE4uosroBgEAxRVuZSGw; path=/; expires=Thu, 31-Oct-24 21:53:56 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=JagmGO7N2C09_xe3yRQSwrRhaRcVzclU7bATv6OvrYo-1730409836453-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8db6b5743fd82f3f-LAX'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-10-31 14:23:56,622 INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-10-31 14:23:56,622 DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-10-31 14:23:56,623 DEBUG - receive_response_body.complete
2024-10-31 14:23:56,623 DEBUG - response_closed.started
2024-10-31 14:23:56,623 DEBUG - response_closed.complete
2024-10-31 14:23:56,623 DEBUG - HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers([('date', 'Thu, 31 Oct 2024 21:23:56 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-wbjdefig2magqgot38fgvt8p'), ('openai-processing-ms', '2698'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '10000'), ('x-ratelimit-remaining-requests', '9999'), ('x-ratelimit-remaining-tokens', '9159'), ('x-ratelimit-reset-requests', '8.64s'), ('x-ratelimit-reset-tokens', '5.046s'), ('x-request-id', 'req_0f219c9ba15b999ec562c928fcf0bd16'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=sozdZa5I8bmhvXm1yfV9y78dn462lgKeInfhtd6zEIQ-1730409836-1.0.1.1-uhB86RBRbc2JHec4XttFIKfilIhfSUe3iebh7ijw.giEMgc4HzrSfchCfZFUrG_uu7jE4uosroBgEAxRVuZSGw; path=/; expires=Thu, 31-Oct-24 21:53:56 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=JagmGO7N2C09_xe3yRQSwrRhaRcVzclU7bATv6OvrYo-1730409836453-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '8db6b5743fd82f3f-LAX'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=":443"; ma=86400')])
2024-10-31 14:23:56,624 DEBUG - request_id: req_0f219c9ba15b999ec562c928fcf0bd16
2024-10-31 14:23:56,631 DEBUG - GPT-4 response (variant 1): {"kubectl_command": "kubectl get svc -l app=harbor -o=jsonpath='{.items[0].metadata.namespace}' --kubeconfig ~/.kube/config"}
2024-10-31 14:23:56,631 INFO - Executing kubectl command: kubectl get svc -l app=harbor -o=jsonpath='{.items[0].metadata.namespace}' --kubeconfig ~/.kube/config
2024-10-31 14:23:56,721 ERROR - Error executing kubectl command: Command 'kubectl get svc -l app=harbor -o=jsonpath='{.items[0].metadata.namespace}' --kubeconfig ~/.kube/config' returned non-zero exit status 1.
2024-10-31 14:23:56,721 WARNING - Command failed with error: Error executing kubectl command: Command 'kubectl get svc -l app=harbor -o=jsonpath='{.items[0].metadata.namespace}' --kubeconfig ~/.kube/config' returned non-zero exit status 1.. Retrying with next variant.
2024-10-31 14:23:56,726 DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are an AI assistant that interprets Kubernetes-related queries and returns structured JSON responses.\nYour goal is to generate accurate \'kubectl\' commands for Kubernetes clusters when possible. Each command should directly retrieve the requested data if available.\nIf a query does not require a Kubernetes command, provide a general response in JSON format.\n\nReturn responses in this JSON format:\n- \'kubectl_command\': The kubectl command to run to satisfy the query.\n\nIf no command is possible, respond instead with:\n- \'general_response\': A concise answer to the question or helpful guidance.\n\nGuidelines for generating commands:\n1. **Counting Resources**:\n   - Avoid using `wc -l` for counting, as it can be inaccurate. Instead, retrieve JSON output and count items programmatically.\n   - Example: For \'How many pods are there?\', respond with:\n     {"kubectl_command": "kubectl get pods --all-namespaces -o json --kubeconfig ~/.kube/config | jq \'.items | length\'"}\n\n2. **Querying Resource Status**:\n   - Use `-o=jsonpath` to query specific fields like `.status`, `.spec`, or `.metadata`. For instance, to get the status of a service or pod, target `.status.phase` or `.status.loadBalancer` as appropriate.\n   - Example: For \'What is the status of harbor registry?\', respond with:\n     {"kubectl_command": "kubectl get svc harbor-registry -o=jsonpath=\'{.status}\' --kubeconfig ~/.kube/config"}\n\n3. **Accessing IPs and Ports**:\n   - To get a pod or service\'s IP, use `.status.podIP` or `.status.loadBalancer.ingress[0].ip`.\n   - For container or service ports, access `.spec.ports[]` and look for `targetPort` or `containerPort`.\n   - Example: For \'What is the container port for harbor-core?\', respond with:\n     {"kubectl_command": "kubectl get pods -l app=harbor-core -o=jsonpath=\'{.items[0].spec.containers[0].ports[0].containerPort}\' --kubeconfig ~/.kube/config"}\n\n4. **Handling Probes (Readiness/Liveness)**:\n   - Use `.spec.containers[].readinessProbe` or `.livenessProbe` for probe paths, often located under `httpGet.path`.\n   - Example: For \'What is the readiness probe path for the harbor core?\', respond with:\n     {"kubectl_command": "kubectl get pods -l app=harbor-core -o=jsonpath=\'{.items[0].spec.containers[0].readinessProbe.httpGet.path}\' --kubeconfig ~/.kube/config"}\n\n5. **Accessing PostgreSQL Configuration Data**:\n   - For questions requiring data outside of kubectl commands (e.g., environment variables or database configuration), use `kubectl exec` to access pods directly.\n   - Example: For \'What is the name of the database in PostgreSQL used by harbor?\', respond with:\n     {"kubectl_command": "kubectl exec -n harbor $(kubectl get pod -n harbor -l app=harbor-database -o jsonpath=\'{.items[0].metadata.name}\' --kubeconfig ~/.kube/config) --kubeconfig ~/.kube/config -- printenv POSTGRES_DB"}\n\nRespond only with the JSON response without extra commentary.\n\nEnsure each command includes `--kubeconfig ~/.kube/config` for specificity in environment handling.\nBe concise, and validate JSON paths to ensure data accuracy.'}, {'role': 'user', 'content': "Interpret this query: Which namespace is the harbor service deployed to?\n\nNote: The previous command attempt failed with the following error:\nError executing kubectl command: Command 'kubectl get svc -l app=harbor -o=jsonpath='{.items[0].metadata.namespace}' --kubeconfig ~/.kube/config' returned non-zero exit status 1."}], 'model': 'gpt-4', 'max_tokens': 100, 'temperature': 0.3}}
2024-10-31 14:23:56,727 DEBUG - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2024-10-31 14:23:56,728 DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-10-31 14:23:56,728 DEBUG - send_request_headers.complete
2024-10-31 14:23:56,728 DEBUG - send_request_body.started request=<Request [b'POST']>
2024-10-31 14:23:56,728 DEBUG - send_request_body.complete
2024-10-31 14:23:56,728 DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-10-31 14:23:59,510 DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 31 Oct 2024 21:23:59 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-wbjdefig2magqgot38fgvt8p'), (b'openai-processing-ms', b'2657'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'10000'), (b'x-ratelimit-remaining-requests', b'9998'), (b'x-ratelimit-remaining-tokens', b'8840'), (b'x-ratelimit-reset-requests', b'14.175s'), (b'x-ratelimit-reset-tokens', b'6.957s'), (b'x-request-id', b'req_d4e57e07d6f5bcfa7470428676d723ab'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8db6b58798542f3f-LAX'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-10-31 14:23:59,511 INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-10-31 14:23:59,511 DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-10-31 14:23:59,512 DEBUG - receive_response_body.complete
2024-10-31 14:23:59,512 DEBUG - response_closed.started
2024-10-31 14:23:59,512 DEBUG - response_closed.complete
2024-10-31 14:23:59,512 DEBUG - HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Thu, 31 Oct 2024 21:23:59 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-wbjdefig2magqgot38fgvt8p', 'openai-processing-ms': '2657', 'openai-version': '2020-10-01', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '10000', 'x-ratelimit-remaining-requests': '9998', 'x-ratelimit-remaining-tokens': '8840', 'x-ratelimit-reset-requests': '14.175s', 'x-ratelimit-reset-tokens': '6.957s', 'x-request-id': 'req_d4e57e07d6f5bcfa7470428676d723ab', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '8db6b58798542f3f-LAX', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2024-10-31 14:23:59,513 DEBUG - request_id: req_d4e57e07d6f5bcfa7470428676d723ab
2024-10-31 14:23:59,514 DEBUG - GPT-4 response (variant 2): {"kubectl_command": "kubectl get svc --all-namespaces -l app=harbor -o=jsonpath='{.items[0].metadata.namespace}' --kubeconfig ~/.kube/config"}
2024-10-31 14:23:59,514 INFO - Executing kubectl command: kubectl get svc --all-namespaces -l app=harbor -o=jsonpath='{.items[0].metadata.namespace}' --kubeconfig ~/.kube/config
2024-10-31 14:23:59,598 DEBUG - kubectl command output: harbor
2024-10-31 14:23:59,598 INFO - Generated answer: harbor
2024-10-31 14:23:59,599 INFO - 127.0.0.1 - - [31/Oct/2024 14:23:59] "POST /query HTTP/1.1" 200 -
